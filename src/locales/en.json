{
  "nav": {
    "products": "Products",
    "gpus": "GPUs",
    "studio": "Studio",
    "pricing": "Pricing",
    "developers": "Developers",
    "company": "Company",
    "login": "Login",
    "contactSales": "Contact Sales"
  },
  "products": {
    "gpuCompute": "GPU Compute",
    "clusterEngine": "Cluster Engine",
    "inferenceEngine": "Inference Engine",
    "modelLibrary": "Model Library"
  },
  "gpus": {
    "h200": "NVIDIA H200",
    "gb200": "NVIDIA GB200 NVL72",
    "hgxb200": "NVIDIA HGX™ B200"
  },
  "developers": {
    "demoApps": "Demo Apps",
    "docsHub": "Docs Hub"
  },
  "company": {
    "aboutUs": "About Us",
    "blog": "Blog",
    "discord": "Discord",
    "partners": "Partners",
    "careers": "Careers"
  },
  "common": {
    "learnMore": "Learn More",
    "getStarted": "Get Started",
    "deployNow": "Deploy Now",
    "applyNow": "Apply Now",
    "joinNow": "Join Now",
    "subscribe": "Subscribe",
    "new": "New",
    "viewModel": "View Model",
    "contactSales": "Contact Sales",
    "seeModelList": "See Model List",
    "showingModels": "Showing {{count}} models",
    "noModelsFound": "No models found matching your criteria.",
    "active": "Active",
    "viewPricing": "View Pricing",
    "watchStory": "Watch Story",
    "viewAllArticles": "View all articles",
    "submit": "Submit",
    "startNow": "Start Now",
    "reserve": "Reserve Now"
  },
  "footer": {
    "subscribeNewsletter": "Subscribe to our newsletter",
    "enterEmail": "Enter email",
    "gpuCloud": "GPU Cloud",
    "clusterEngine": "Cluster Engine",
    "inferenceEngine": "Inference Engine",
    "pricing": "Pricing",
    "modelLibrary": "Model Library",
    "glossary": "Glossary",
    "blog": "Blog",
    "careers": "Careers",
    "aboutUs": "About Us",
    "partners": "Partners",
    "contactUs": "Contact Us",
    "privacyPolicy": "Privacy Policy",
    "termsOfUse": "Terms of Use",
    "allRightsReserved": "© 2025 All Rights Reserved."
  },
  "home": {
    "heroTitle": "Build AI Without Limits",
    "heroSubtitle": "We build complete GPU infrastructure so you can focus on what really matters — your AI.",
    "getStarted": "Get Started",
    "viewPricing": "View Pricing",
    "foundationTitle": "The Foundation for Your AI Success",
    "foundationSubtitle": "— Powered by GPU Cloud Solutions",
    "foundationDescription": "GMI Cloud provides everything you need to build scalable AI solutions — running entirely on GMI-owned, AI-optimized datacenters. From high-performance inference and containerized ops to on-demand access to top-tier GPUs for training and inference, we control the full stack.",
    "inferenceTitle": "Inference Engine",
    "inferenceDescription1": "GMI Cloud Inference Engine provides developers with the ultra-fast speed and flexible scalability needed to run AI models, designed for extremely low latency, high concurrency processing, and top-tier performance optimization.",
    "inferenceDescription2": "Through GMI Cloud's global GPU node layout, developers can instantly deploy models, scale computing power anytime and anywhere, automatically adjust computing loads, significantly reduce costs and improve performance.",
    "inferenceModelsLabel": "Easily run market-leading AI models",
    "clusterTitle": "Cluster Engine",
    "clusterDescription": "GMI Cloud's Cluster Engine is your all-in-one AI cloud management platform, designed to streamline GPU resource coordination, workload scheduling, and cost optimization.",
    "gpuTitle": "GPUs",
    "gpuDescription": "GMI Cloud offers a diverse portfolio of NVIDIA GPUs, designed for AI/ML training, inference, and data analytics. Our GPU infrastructure is purpose-built for the most demanding AI workloads.",
    "caseStudyTitle": "Proven Results",
    "caseStudySubtitle": "Our customers achieve measurable improvements in cost, performance, and time-to-market.",
    "caseStudyQuote": "NexusAI transformed how we deploy AI models. What used to take weeks now takes hours.",
    "costSavings": "Cost Savings",
    "costSavingsDesc": "Average reduction in infrastructure costs",
    "fasterDeployment": "Faster Deployment",
    "fasterDeploymentDesc": "From weeks to hours for new GPU clusters",
    "newsTitle": "Latest Insights",
    "newsSubtitle": "Stay updated with the latest in AI infrastructure and GPU computing.",
    "ctaTitle": "Ready to Deploy AI at Scale?",
    "ctaSubtitle": "Join thousands of companies using NexusAI to power their AI infrastructure. Get started in minutes."
  },
  "faq": {
    "title": "Frequently Asked Questions",
    "subtitle": "Everything you need to know about our GPU infrastructure platform.",
    "items": {
      "q1": { "question": "What GPU models do you offer?", "answer": "We offer the latest NVIDIA GPUs including H100, A100, and L40S. All clusters are equipped with NVLink and InfiniBand for maximum performance." },
      "q2": { "question": "How quickly can I deploy a new cluster?", "answer": "Most clusters can be provisioned within hours, not weeks. Our automated infrastructure allows for rapid scaling." },
      "q3": { "question": "What security certifications do you have?", "answer": "We maintain SOC 2 Type II, HIPAA, and GDPR compliance. All data is encrypted at rest and in transit." },
      "q4": { "question": "Can I bring my own models?", "answer": "Absolutely. Our platform supports any model format including PyTorch, TensorFlow, and ONNX." },
      "q5": { "question": "How does billing work?", "answer": "We offer flexible pricing including on-demand, reserved capacity, and committed use discounts." }
    }
  },
  "news": {
    "articles": {
      "article1": { "title": "Scaling AI Infrastructure: Lessons from 10,000 GPUs", "excerpt": "How enterprise teams are managing GPU clusters at unprecedented scale.", "category": "Engineering" },
      "article2": { "title": "The Future of Inference: H100 vs A100 Benchmarks", "excerpt": "Real-world performance comparisons for production AI workloads.", "category": "Research" },
      "article3": { "title": "Cost Optimization Strategies for AI Teams", "excerpt": "Practical tips to reduce your GPU infrastructure costs by up to 50%.", "category": "Business" }
    }
  },
  "cluster": {
    "centralizedManagement": "Centralized Management",
    "centralizedManagementDesc": "Unify your GPU resources across multiple regions and manage workloads from a single, intuitive dashboard.",
    "realtimeDashboard": "Real-Time Dashboard",
    "realtimeDashboardDesc": "Monitor performance metrics, track resource utilization, and get instant alerts for any anomalies.",
    "accessManagement": "Access Management",
    "accessManagementDesc": "Securely manage user permissions and control access to GPU resources with role-based access controls.",
    "activeGPUs": "Active GPUs",
    "lastUpdated": "Last updated: Just now",
    "avgGPUUsage": "Avg. GPU Usage",
    "activeNodes": "Active Nodes",
    "memoryUsed": "Memory Used",
    "uptime": "Uptime",
    "teamMembers": "Team Members",
    "invite": "+ Invite"
  },
  "gpu": {
    "topTierGPUs": "Top Tier GPUs",
    "topTierGPUsDesc": "Access the latest NVIDIA H100 and A100 GPUs with guaranteed availability and competitive pricing.",
    "infinibandNetworking": "InfiniBand Networking",
    "infinibandNetworkingDesc": "Ultra-low latency interconnects with 400Gb/s bandwidth for distributed training and real-time inference.",
    "secureScalable": "Secure and Scalable",
    "secureScalableDesc": "Enterprise-grade security with SOC 2 Type II, HIPAA, and GDPR compliance. Scale from 1 to 1000+ GPUs."
  },
  "pricing": {
    "title": "Pricing",
    "subtitle": "Competitive, transparent pricing with flexible billing — tailored for modern AI deployment",
    "gpuPlans": {
      "h200": { "label": "Highest Performance", "description": "For large-scale training needs, the most powerful compute option. Equipped with HBM3e memory for unmatched AI training performance.", "cta": "Contact Us" },
      "h100": { "label": "Performance & Cost Balance", "description": "Ideal for high-volume inference tasks with cost-effective optimization. 80GB HBM3 memory for enterprise-grade AI deployment.", "cta": "Deploy Now" },
      "blackwell": { "label": "Coming Soon", "headline": "Reserve Now", "description": "Blackwell launches in 2025 with higher efficiency and AI optimization features for next-gen AI workloads.", "cta": "Reserve Now" }
    },
    "serviceEngine": {
      "title": "Supercharge Your GPU Cloud Computing",
      "subtitle": "Break through performance bottlenecks and accelerate AI training and inference.",
      "serviceType": "Service Type",
      "inference": { "title": "Inference Engine", "description": "GMI Cloud's inference service is optimized for high-throughput and low-latency scenarios. The optimized inference stack ensures models run at peak performance.", "cta": "Learn More" },
      "cluster": { "title": "Cluster Engine", "description": "GMI Cluster service is optimized for distributed training and large-scale computing. InfiniBand high-speed networking enables multi-node coordination.", "cta": "Start Deploying" }
    },
    "cta": { "title": "Not sure which product fits your needs? Let's talk.", "subtitle": "Our team is here to help you choose the right GPU cloud solution and answer any questions about performance, pricing, or scaling." },
    "faq": {
      "title": "Frequently Asked Questions",
      "subtitle": "Have questions about our services? Check out these common questions.",
      "items": {
        "q1": { "question": "What types of GPUs are available?", "answer": "We offer the latest NVIDIA GPUs including H200 (with HBM3e memory), H100 80GB HBM3, and A100 80GB HBM2e. All configurations include NVLink and InfiniBand." },
        "q2": { "question": "How does billing and payment work?", "answer": "We use an on-demand billing model calculated per GPU-hour. You can choose prepaid or postpaid plans with credit card, bank transfer, and more." },
        "q3": { "question": "Are volume discounts available?", "answer": "Yes! We offer flexible discount programs including long-term subscription discounts (up to 40%), volume usage discounts, and enterprise-specific pricing." },
        "q4": { "question": "Can I adjust resources anytime?", "answer": "Absolutely. You can increase or decrease GPU resources anytime via the console or API. Our elastic architecture supports instant scaling." },
        "q5": { "question": "How do I get started?", "answer": "Getting started is simple. Contact our sales team for a consultation and we'll help determine the right configuration for your needs." }
      }
    }
  },
  "gpuPage": {
    "heroTitle": "GPU Compute Rental",
    "heroSubtitle": "Bare metal servers with full cloud integration, offered at the most competitive prices.",
    "features": {
      "onDemand": {
        "title": "Flexible On-Demand GPU Service",
        "description": "Instantly access NVIDIA GPU compute power and rapidly deploy your workloads. Our scalable platform lets you freely adjust resources to perfectly support AI and machine learning tasks. Affordable pricing with no long-term contracts gives you maximum flexibility without upfront costs."
      },
      "hardware": {
        "title": "Top-Tier Hardware Specs",
        "description": "Equipped with 3.2 Tbps InfiniBand networking for ultra-fast distributed training connections; powered by advanced NVIDIA H100 GPU training clusters for ultimate compute power. Simple SSH access and dataset downloads let you start your AI journey immediately."
      },
      "privateCloud": {
        "title": "Enterprise Private Cloud",
        "description": "GMI Cloud provides dedicated cloud environments for enterprises, ensuring end-to-end security and data isolation. Customizable configurations meet your unique IT policies. Build your way with complete control and flexibility, seamlessly switching between cloud and on-premises environments."
      },
      "security": {
        "title": "Secure Network Architecture",
        "description": "Built on high-performance InfiniBand networking for stable, efficient cross-region connectivity. Applications run with optimal performance while strict network access controls maintain security."
      },
      "selectHardware": "Select hardware resources",
      "numGpus": "Number of GPUs",
      "numCpus": "Number of CPUs",
      "ram": "RAM",
      "ephemeralStorage": "Ephemeral Storage"
    }
  },
  "h200Page": {
    "hero": {
      "price": "Starts from $2.15 / GPU-hour",
      "title": "Accelerate AI Innovation with NVIDIA H200 Cloud GPUs",
      "description": "The NVIDIA H200 Tensor Core GPU supercharges generative AI and high-performance computing (HPC) workloads with game-changing performance and memory capabilities. With 141GB of HBM3e memory and 4.8TB/s bandwidth, it delivers unprecedented throughput for large language model inference.",
      "cta": "Deploy Now"
    },
    "features": {
      "memory": {
        "title": "Higher Memory Capacity",
        "description": "The H200 features 141GB of HBM3e memory, nearly double the capacity of the H100, enabling the processing of larger models and datasets without memory constraints."
      },
      "bandwidth": {
        "title": "Increased Memory Bandwidth",
        "description": "With 4.8 TB/s of memory bandwidth, the H200 offers 1.4x more bandwidth than the H100, significantly speeding up data transfer and processing tasks."
      },
      "performance": {
        "title": "Enhanced AI Performance",
        "description": "The H200 is optimized for generative AI and large language models, delivering up to 1.9x faster inference performance for models like Llama 2 70B."
      }
    },
    "chart": {
      "title": "Performance Comparison: H100 vs H200",
      "benchmark1": "Llama 2 70B Inference (tokens/sec)",
      "benchmark2": "Llama 2 70B (FP8) Inference (tokens/sec)",
      "resultsTitle": "Benchmark Results",
      "resultsP1": "The NVIDIA H200 demonstrates significant performance improvements over the H100 in real-world AI inference workloads. In Llama 2 70B inference benchmarks, the H200 delivers",
      "resultsP1Highlight": "1.4x faster throughput",
      "resultsP1End": "compared to the H100.",
      "resultsP2": "When utilizing FP8 precision, the performance gains are even more dramatic. The H200 achieves",
      "resultsP2Highlight": "1.9x faster inference",
      "resultsP2End": "for Llama 2 70B models, making it the ideal choice for production AI deployments requiring both speed and efficiency.",
      "resultsP3": "These improvements are driven by the H200's enhanced HBM3e memory architecture, delivering 4.8 TB/s of bandwidth – crucial for memory-intensive large language model operations."
    },
    "marketing": {
      "title": "Future-Proof Your AI with GMI Cloud and the H200",
      "description": "As AI models continue to grow in size and complexity, the NVIDIA H200 provides the headroom you need to stay ahead. With its massive 141GB memory capacity and industry-leading bandwidth, the H200 on GMI Cloud ensures your infrastructure can handle tomorrow's AI workloads today. Scale from development to production seamlessly with our enterprise-grade cloud platform.",
      "cta": "Reserve Now"
    },
    "cta": {
      "title": "Don't miss out on the opportunity to deploy the most powerful GPU resources in the world.",
      "button": "Contact Us"
    }
  },
  "gb200Page": {
    "hero": {
      "title": "Next-Gen AI Acceleration with NVIDIA GB200 NVL72",
      "description": "Advanced AI infrastructure designed for the most demanding enterprise workloads. The GB200 NVL72 combines Grace CPUs with Blackwell GPUs for unprecedented performance.",
      "cta": "Get Started"
    },
    "features": {
      "sectionTitle": "Empowering AI Innovation",
      "sectionDescription": "The NVIDIA GB200 NVL72 represents a quantum leap in AI computing, bringing together the most advanced GPU architecture with purpose-built infrastructure for enterprise-scale deployments.",
      "items": {
        "performance": {
          "title": "Unmatched AI Performance",
          "description": "Combines Grace CPUs with Blackwell GPUs for unprecedented compute density and efficiency, delivering breakthrough performance for AI training and inference."
        },
        "dataProcessing": {
          "title": "Next-Level Data Processing for Enterprise AI",
          "description": "Handles multi-trillion parameter models with ease, enabling organizations to tackle the most complex AI challenges without infrastructure limitations."
        },
        "scalability": {
          "title": "Next-Level Scalability for LLM and AI Workloads",
          "description": "Scale from single node to thousands with NVLink interconnect, providing seamless expansion as your AI requirements grow."
        },
        "efficiency": {
          "title": "Energy-Efficient Architecture",
          "description": "Optimized power consumption per FLOP means lower operational costs while maintaining peak performance for sustainable AI infrastructure."
        }
      },
      "cta": "Contact Sales"
    },
    "solutions": {
      "title": "Custom GPU Cloud Solutions Built for Your AI Needs",
      "training": {
        "title": "AI Training Infrastructure",
        "description": "Dedicated infrastructure for AI training with high-speed NVLink connectivity, enabling distributed training across thousands of GPUs with minimal latency."
      },
      "inference": {
        "title": "Production Inference",
        "description": "Low-latency inference solutions optimized for production AI applications, delivering real-time responses for mission-critical workloads."
      },
      "scaling": {
        "title": "Flexible Scaling",
        "description": "Flexible scaling options from single GPU to multi-node clusters, with on-demand provisioning that adapts to your workload requirements."
      }
    },
    "futureProof": {
      "title": "Future-Proof Your AI with GMI Cloud and the GB200 NVL72",
      "description": "As AI models continue to grow in complexity, the GB200 NVL72 ensures your infrastructure stays ahead of the curve. With its groundbreaking architecture and seamless scalability, you can confidently invest in tomorrow's AI capabilities today. GMI Cloud provides the platform to harness this power with enterprise-grade reliability and support.",
      "cta": "Learn More"
    },
    "contact": {
      "title": "Contact us",
      "subtitle": "Get in touch with our team for more information",
      "name": "Name",
      "company": "Company",
      "email": "Email",
      "phone": "Phone Number",
      "message": "Message",
      "submit": "Send Message",
      "successTitle": "Message sent!",
      "successMessage": "We'll get back to you as soon as possible."
    }
  },
  "hgxb200Page": {
    "hero": {
      "title": "Unleash the Power of NVIDIA HGX™ B200",
      "description": "Top-tier performance for complex models and enterprise-scale AI deployments. The ultimate 8-GPU platform for AI training and inference.",
      "cta": "Contact Sales",
      "learnMore": "Learn more"
    },
    "features": {
      "sectionTitle": "Next-Generation AI Compute",
      "sectionDescription": "The NVIDIA HGX B200 platform represents the pinnacle of AI infrastructure, combining 8 Blackwell architecture GPUs with advanced NVLink interconnect technology. Designed for organizations pushing the boundaries of artificial intelligence, from training frontier models to deploying real-time inference at scale.",
      "items": {
        "performance": {
          "title": "Optimized GPU Performance for AI Training & Inference",
          "description": "The HGX B200 delivers unprecedented compute density with 8 Blackwell GPUs connected via 5th-generation NVLink, providing 1.8TB/s of GPU-to-GPU bandwidth for seamless parallel processing."
        },
        "architecture": {
          "title": "Highly-Scaled Architecture for Demanding AI Workloads",
          "description": "Purpose-built for multi-trillion parameter models and enterprise AI deployments. The unified memory architecture enables efficient handling of the largest foundation models."
        },
        "scalability": {
          "title": "Seamless AI Scalability",
          "description": "Connect multiple HGX B200 systems for rack-scale AI supercomputing. NVLink Switch enables linear scaling across thousands of GPUs for the most demanding training workloads."
        }
      },
      "datasheet": "View NVIDIA HGX B200 Platform Datasheet"
    },
    "solutions": {
      "title": "Flexible GPU Solutions Tailored to Your AI Needs",
      "onDemand": {
        "title": "On-Demand Access",
        "description": "Flexible hourly billing for experimentation and development workloads with instant provisioning. Perfect for teams exploring new model architectures or running periodic training jobs."
      },
      "reserved": {
        "title": "Reserved Capacity",
        "description": "Discounted pricing for long-term AI projects with guaranteed availability and performance. Ideal for production workloads requiring predictable compute resources."
      },
      "dedicated": {
        "title": "Dedicated Clusters",
        "description": "Custom configurations of HGX B200 systems for large-scale training with dedicated networking and storage. Designed for organizations training frontier AI models."
      }
    },
    "elevate": {
      "title": "Elevate Your AI Capabilities with GMI Cloud and NVIDIA HGX B200",
      "description": "Leverage the world's most powerful AI platform with GMI Cloud's enterprise infrastructure. Get immediate access to HGX B200 systems without capital investment, backed by 24/7 support and optimized MLOps tooling for accelerated time-to-value.",
      "cta": "Request Access"
    },
    "contact": {
      "title": "Contact us",
      "subtitle": "Get in touch with our team for more information",
      "name": "Name",
      "company": "Company",
      "email": "Email",
      "phone": "Phone Number",
      "message": "Message",
      "submit": "Send Message",
      "successTitle": "Message sent!",
      "successMessage": "We'll get back to you as soon as possible."
    }
  },
  "contact": {
    "title": "Contact Us",
    "subtitle": "Get in touch with our team for more information and support.",
    "sales": { "title": "Sales", "description": "Contact our sales team to learn more about our offerings and pricing." },
    "support": { "title": "Help & Support", "description": "Reach out to our support team for technical assistance and customer service." },
    "media": { "title": "Media & Press", "description": "For media inquiries, connect with our press team." },
    "form": {
      "firstName": "First Name",
      "lastName": "Last Name",
      "email": "Email",
      "interestType": "Interest Type",
      "selectOne": "Select one...",
      "gpuCloud": "GPU Cloud Services",
      "inference": "Inference Engine",
      "cluster": "Cluster Engine",
      "enterprise": "Enterprise Solutions",
      "partnership": "Partnership",
      "other": "Other",
      "company": "Company Name",
      "submit": "Submit",
      "successTitle": "Message sent!",
      "successMessage": "We'll get back to you within 24 hours."
    }
  },
  "about": {
    "kicker": "About GMI Cloud",
    "heroTitle": "Building AI Infrastructure, Empowering Possibilities",
    "heroSubtitle": "We are dedicated to building the most advanced GPU cloud computing platform, accelerating global AI innovation.",
    "stats": { "year": "Year", "coreMembers": "Core Members", "globalOffices": "Global Offices" }
  },
  "studio": { "heroTitle": "Creativity That Flows.", "heroSubtitle": "Your intelligent canvas for dreaming, iterating, and shaping workflows.", "startNow": "Start Now" },
  "models": {
    "deepseekR1": { "description": "Open-source reasoning model, rivaling OpenAI o1, excelling in math, coding, and multi-step reasoning." },
    "deepseekR1Distill": { "description": "Free endpoint to experience powerful reasoning model, this distilled version retains excellent reasoning capabilities." },
    "llama33": { "description": "Open-source reasoning model, supports multi-language dialogue optimization, specifically tuned for dialogue fluency." },
    "free": "Free"
  },
  "modelLibrary": { "heroTitle": "Model Library", "heroSubtitle": "Explore our curated library of powerful open-source models and accelerate your AI apps.", "filters": { "all": "All", "llm": "LLM", "vision": "Vision", "video": "Video", "embedding": "Embedding" }, "searchPlaceholder": "Search models...", "ctaTitle": "Not sure which product fits your needs? Let's talk.", "ctaSubtitle": "Our team is here to help you choose the right infrastructure and models for your AI applications.", "types": { "textGeneration": "Text Generation", "codeGeneration": "Code Generation", "imageGeneration": "Image Generation", "visionLanguage": "Vision Language", "textEmbedding": "Text Embedding" } },
  "inferenceEngine": { "heroTitle": "NexusAI Cloud Inference Engine", "heroSubtitle": "Run and scale generative AI models with ease.", "cubeLabel": "Inference", "cubeSubLabel": "Engine", "smarterWayTitle": "A Smarter Way to Inference", "features": { "deployment": { "title": "Rapid Deployment, Zero Hassle", "description": "Scale instantly with fully managed infrastructure." }, "efficiency": { "title": "Optimized for Efficiency", "description": "Advanced GPU optimization and model caching minimize latency." } }, "modelShowcase": { "title": "Pre-Built AI Models for Fast Inference", "subtitle": "Choose from our curated library of pre-optimized models or bring your own", "inference": "Inference", "viewAllModels": "View all models in our library →", "models": { "llama2": { "name": "Llama 2", "description": "Open-source large language model for text generation and chat" }, "stableDiffusion": { "name": "Stable Diffusion", "description": "State-of-the-art image generation model" }, "mpt7b": { "name": "MPT-7B", "description": "Efficient transformer model for commercial use" } } }, "scaling": { "kicker": "Auto-Scale", "title": "Effortless Scaling for Your AI Workloads", "description": "Scale from zero to thousands of concurrent requests automatically.", "dynamicScaling": { "title": "Dynamic Scaling", "description": "Automatically provisions GPUs as demand increases." }, "flexibility": { "title": "Blazing Flexibility", "description": "Choose your minimum and maximum replicas." }, "cta": "Get Started Now" }, "monitoring": { "kicker": "Monitor", "title": "Real-Time AI Performance Monitoring", "description": "Track latency, throughput, and error rates in real-time.", "totalInferences": "Total Inferences", "avgLatency": "Avg Latency", "throughput": "Throughput", "uptime": "Uptime", "costPerInference": "Cost/1K inferences" }, "cta": { "title": "Start Inferencing Now", "subtitle": "Deploy your first model in minutes." }, "faq": { "title": "Frequently asked questions", "subtitle": "Get quick answers to common questions about Inference Engine", "items": { "q1": { "question": "What is the NexusAI Cloud Inference Engine?", "answer": "The Inference Engine is our managed platform for deploying and scaling AI models in production." }, "q2": { "question": "How fast is deployment?", "answer": "Deployment typically takes under 2 minutes for pre-built models." }, "q3": { "question": "How does it optimize performance and cost?", "answer": "Our engine uses GPU batching, model caching, and intelligent request routing." }, "q4": { "question": "How does auto-scaling handle fluctuating traffic?", "answer": "The auto-scaler monitors request queue depth, latency, and GPU utilization in real-time." }, "q5": { "question": "Do I get built-in monitoring?", "answer": "Yes! Every deployment includes a comprehensive dashboard with real-time metrics." } } } },
  "clusterEngine": { "heroTitle": "NexusAI Cloud Cluster Engine", "heroSubtitle": "Automated orchestration for AI workloads at scale.", "deployNow": "Deploy Now", "integrationTitle": "Your AI Control Plane", "integrationSubtitle": "Seamlessly integrate with your existing tools", "features": { "scaling": { "title": "Efficient Scaling", "description": "Automatically scale your GPU clusters." }, "monitoring": { "title": "Real-time Monitoring", "description": "Monitor every aspect of your cluster." }, "analytics": { "title": "Usage Analytics", "description": "Gain deep insights into your compute usage." }, "resources": { "title": "Resource Management", "description": "Fine-grained control over every GPU, node, and container." }, "security": { "title": "Enterprise Security", "description": "Bank-grade security for your AI workloads." } }, "ctaTitle": "Ready to scale your AI infrastructure?", "ctaSubtitle": "Get started with Cluster Engine today.", "darkCtaTitle": "Manage Workloads Effortlessly", "darkCtaSubtitle": "Deploy, monitor, and scale your AI workloads.", "faqTitle": "Frequently asked questions", "faqSubtitle": "Everything you need to know about Cluster Engine" },
  "careers": { "heroTitle": "Build the Future of AI with NexusAI", "heroSubtitle": "We're shaping the future of AI and high-performance computing.", "joinButton": "Join NexusAI", "seePositions": "See Open Positions", "valuesKicker": "We're offering the chance to shape the future of AI.", "values": { "engineering": { "title": "Engineering Excellence", "description": "Build complex, cutting-edge technology." }, "business": { "title": "Business at the Forefront", "description": "Join a team of bold thinkers." }, "startup": { "title": "Startup Energy, Big Ambitions", "description": "We move fast and value fresh ideas." } }, "officesKicker": "Our team is at the forefront of global AI development.", "joinTeamTitle": "Join a Global Team of Innovators", "joinTeamSubtitle": "We want to bring together bold thinkers from around the world." },
  "partners": { "heroTitle": "Join the NexusAI Partner Program", "heroSubtitle": "Join a robust ecosystem of technology leaders.", "stats": { "founded": "Founded", "partners": "Partners", "gpuHours": "GPU Hours" }, "diverseTitle": "Diverse Partners, Tailored Success", "whyJoinTitle": "Why Join the NexusAI Partner Program?", "benefits": { "access": { "title": "Exclusive Access", "description": "Get priority access to new GPU infrastructure." }, "marketing": { "title": "Co-Marketing", "description": "Amplify your brand through joint marketing campaigns." }, "support": { "title": "Technical Support", "description": "Receive dedicated technical resources." } }, "newProductTitle": "New product", "faqTitle": "Frequently asked questions" },
  "blog": { "heroTitle": "NexusAI Blog", "heroSubtitle": "Discover the latest news, updates, and insights from our team." },
  "gpuFaq": {
    "title": "Frequently Asked Questions",
    "subtitle": "Get quick answers to common questions",
    "items": {
      "q1": {
        "question": "What types of GPUs do you offer?",
        "answer": "We offer the latest NVIDIA GPUs including H100 (80GB HBM3 memory), H200 (HBM3e memory), and the upcoming Blackwell series. All configurations include NVLink and InfiniBand for optimal distributed training performance."
      },
      "q2": {
        "question": "How do I manage GPU clusters for distributed training?",
        "answer": "Our platform includes a comprehensive cluster management dashboard. You can configure nodes, monitor utilization, set up networking, and manage jobs through our web console or API. We also support popular orchestration tools like Kubernetes and Slurm."
      },
      "q3": {
        "question": "Which deep learning frameworks are supported? Can I customize?",
        "answer": "We support all major frameworks including PyTorch, TensorFlow, JAX, and ONNX. You can use our pre-configured containers or bring your own Docker images. We also offer optimized versions for maximum GPU utilization."
      },
      "q4": {
        "question": "What are the pricing options? Do you offer cost optimization?",
        "answer": "We offer flexible pricing options including on-demand hourly rates, reserved capacity with significant discounts, and committed use contracts. Our platform includes built-in cost optimization tools to help you minimize spending while maximizing performance."
      }
    }
  },
  "developers": {
    "demoApps": "Demo Apps",
    "docsHub": "Docs Hub",
    "demoHero": {
      "badge": "Demo",
      "title": "Demo Apps for Developers",
      "subtitle": "Explore and test live AI models on GMI Cloud. Build prototypes, run experiments, and integrate generative AI into your apps."
    },
    "demoAppsContent": {
      "ragChatbot": {
        "title": "Multimodal RAG Chatbot",
        "description": "Intelligent multimodal RAG chatbot granting natural Q&A capabilities, generated answers, and interactive visuals for Q&A, summarization, and multimedia workflows."
      },
      "deepResearch": {
        "title": "Deep Research Agent",
        "description": "Long-context agent analyzing sources and producing a structured, citation-based report — a shortcut reasoning for complex research."
      },
      "companyResearch": {
        "title": "Company Research Agent",
        "description": "Specialized agent for company analysis, synthesizing funding, products, competitors, and market position into a narrative basis for business growth."
      }
    },
    "demoTags": {
      "rag": "RAG",
      "chatbot": "Chatbot",
      "qa": "Q&A",
      "pdfUpload": "PDF Upload",
      "multimedia": "Multimedia",
      "knowledgeGrounding": "Knowledge Grounding",
      "aiAssistant": "AI Assistant",
      "research": "Research",
      "longContext": "Long Context",
      "summarization": "Summarization",
      "analysis": "Analysis",
      "citations": "Citations",
      "multiDocument": "Multi-Document",
      "knowledgeSynthesis": "Knowledge Synthesis",
      "companyResearch": "Company Research",
      "businessIntelligence": "Business Intelligence",
      "competitorAnalysis": "Competitor Analysis",
      "webGeneration": "Web Generation",
      "salesEnablement": "Sales Enablement",
      "marketResearch": "Market Research"
    },
    "readyBanner": {
      "title": "Ready to build?",
      "subtitle": "Explore powerful AI models and launch your project in just a few clicks."
    },
    "link": "Link",
    "code": "Code",
    "docsHub": {
      "badge": "Documentation",
      "title": "Docs Hub",
      "subtitle": "Comprehensive documentation, API references, and integration guides to help you build with GMI Cloud. Coming soon.",
      "earlyAccess": "Get Early Access",
      "exploreDemos": "Explore Demo Apps"
    }
  },
  "h200Faq": {
    "title": "Frequently Asked Questions",
    "subtitle": "Everything you need to know about the NVIDIA H200 on GMI Cloud",
    "items": {
      "q1": {
        "question": "What is the NVIDIA H200 GPU offered by GMI Cloud?",
        "answer": "The NVIDIA H200 is the latest high-performance GPU from NVIDIA, featuring 141GB of HBM3e memory and 4.8 TB/s bandwidth. GMI Cloud offers on-demand access to H200 GPUs in our enterprise-grade cloud infrastructure, enabling you to run the most demanding AI and HPC workloads without capital investment."
      },
      "q2": {
        "question": "How does the H200 differ from previous GPU models like the H100?",
        "answer": "The H200 offers nearly double the memory capacity (141GB vs 80GB) and 1.4x more memory bandwidth compared to the H100. In practical benchmarks, this translates to 1.4x-1.9x faster inference performance for large language models like Llama 2 70B."
      },
      "q3": {
        "question": "How does the H200 enhance generative AI and LLM development?",
        "answer": "The H200's massive memory capacity and bandwidth are specifically designed for large language models. It can handle larger batch sizes, longer context windows, and bigger models without memory constraints, enabling faster iteration and more efficient training and inference workflows."
      },
      "q4": {
        "question": "What are the benefits of using the H200 within GMI Cloud?",
        "answer": "GMI Cloud provides instant access to H200 GPUs with pay-as-you-go pricing starting at $2.15/GPU-hour. Benefits include enterprise SLAs, NVLink connectivity for multi-GPU workloads, pre-configured ML environments, and 24/7 technical support."
      },
      "q5": {
        "question": "How can users access the H200 GPU on GMI Cloud?",
        "answer": "Getting started is simple: sign up for a GMI Cloud account, select the H200 GPU configuration that matches your workload needs, and deploy within minutes. Our team is also available to help with custom configurations and enterprise deployments."
      }
    }
  },
  "gb200Faq": {
    "title": "Frequently asked questions",
    "items": {
      "q1": {
        "question": "What is the NVIDIA GB200 NVL72 and why is it unique?",
        "answer": "The NVIDIA GB200 NVL72 is the next generation of AI supercomputing, combining 72 Blackwell GPUs with Grace CPUs in a single rack-scale system. It delivers unprecedented AI training and inference performance with up to 30x better performance per watt compared to previous generations."
      },
      "q2": {
        "question": "What performance improvements does the GB200 NVL72 provide?",
        "answer": "The GB200 NVL72 delivers up to 30x faster AI inference and 4x faster training compared to H100-based systems. With 13.5TB of unified HBM3e memory and 130TB/s of memory bandwidth, it can handle the largest AI models with ease."
      },
      "q3": {
        "question": "How does the GB200 NVL72 support scalability?",
        "answer": "The system uses 5th-generation NVLink with 1.8TB/s GPU-to-GPU bandwidth, enabling seamless scaling across the entire 72-GPU system. Multiple NVL72 racks can be connected via high-speed networking for even larger deployments."
      },
      "q4": {
        "question": "What benefits does the GB200 NVL72 bring when used within GMI Cloud?",
        "answer": "GMI Cloud provides fully managed GB200 NVL72 infrastructure with enterprise SLAs, dedicated support, and flexible deployment options. Users get immediate access without capital investment, along with optimized software stack and MLOps tooling."
      },
      "q5": {
        "question": "How can users access the GB200 NVL72?",
        "answer": "Contact our sales team to discuss your requirements and join our priority access program. We offer flexible deployment options including dedicated clusters, reserved capacity, and on-demand access depending on your needs."
      }
    }
  },
  "hgxb200Faq": {
    "title": "Frequently asked questions",
    "items": {
      "q1": {
        "question": "What is the NVIDIA HGX B200 and what is it used for?",
        "answer": "The NVIDIA HGX B200 is a high-performance AI computing platform featuring 8 Blackwell architecture GPUs on a single baseboard. It's designed for the most demanding AI training and inference workloads, including training large language models, generative AI, and scientific computing applications."
      },
      "q2": {
        "question": "What makes the HGX B200's performance unique?",
        "answer": "The HGX B200 delivers up to 144 petaflops of AI performance with 8 Blackwell GPUs. Each GPU features next-generation Tensor Cores with FP4 support, enabling 4x faster inference compared to previous generations. The unified HBM3e memory pool provides 1.4TB of high-bandwidth memory."
      },
      "q3": {
        "question": "What architectural advantages does the HGX B200 provide?",
        "answer": "The platform features 5th-generation NVLink with 1.8TB/s of GPU-to-GPU bandwidth, enabling all 8 GPUs to work as a unified compute engine. The decompression engine accelerates database queries, while the transformer engine optimizes LLM workloads automatically."
      },
      "q4": {
        "question": "How does the HGX B200 ensure scalability?",
        "answer": "Multiple HGX B200 systems can be connected via NVLink Switch and high-speed networking to create AI supercomputers with thousands of GPUs. The architecture supports linear scaling for both training and inference workloads, making it ideal for frontier AI development."
      },
      "q5": {
        "question": "How can customers access the HGX B200 platform?",
        "answer": "GMI Cloud offers flexible access to HGX B200 infrastructure including on-demand hourly access, reserved capacity with volume discounts, and fully dedicated clusters. Contact our sales team to discuss your requirements and get started with a customized deployment."
      }
    }
  },
  "languages": { "en": "English", "zh-TW": "繁體中文", "ja": "日本語", "ko": "한국어" }
}