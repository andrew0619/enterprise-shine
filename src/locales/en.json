{
  "nav": {
    "products": "Products",
    "gpus": "GPUs",
    "studio": "Studio",
    "pricing": "Pricing",
    "developers": "Developers",
    "company": "Company",
    "login": "Login",
    "contactSales": "Contact Sales"
  },
  "products": {
    "gpuCompute": "GPU Compute",
    "clusterEngine": "Cluster Engine",
    "inferenceEngine": "Inference Engine",
    "modelLibrary": "Model Library"
  },
  "gpus": {
    "h200": "NVIDIA H200",
    "gb200": "NVIDIA GB200 NVL72",
    "hgxb200": "NVIDIA HGX™ B200"
  },
  "developers": {
    "demoApps": "Demo Apps",
    "docsHub": "Docs Hub"
  },
  "company": {
    "aboutUs": "About Us",
    "blog": "Blog",
    "discord": "Discord",
    "partners": "Partners",
    "careers": "Careers"
  },
  "common": {
    "learnMore": "Learn More",
    "getStarted": "Get Started",
    "deployNow": "Deploy Now",
    "applyNow": "Apply Now",
    "joinNow": "Join Now",
    "subscribe": "Subscribe",
    "new": "New",
    "viewModel": "View Model",
    "contactSales": "Contact Sales",
    "seeModelList": "See Model List",
    "showingModels": "Showing {{count}} models",
    "noModelsFound": "No models found matching your criteria.",
    "active": "Active",
    "viewPricing": "View Pricing",
    "watchStory": "Watch Story",
    "viewAllArticles": "View all articles"
  },
  "footer": {
    "subscribeNewsletter": "Subscribe to our newsletter",
    "enterEmail": "Enter email",
    "gpuCloud": "GPU Cloud",
    "clusterEngine": "Cluster Engine",
    "inferenceEngine": "Inference Engine",
    "pricing": "Pricing",
    "modelLibrary": "Model Library",
    "glossary": "Glossary",
    "blog": "Blog",
    "careers": "Careers",
    "aboutUs": "About Us",
    "partners": "Partners",
    "contactUs": "Contact Us",
    "privacyPolicy": "Privacy Policy",
    "termsOfUse": "Terms of Use",
    "allRightsReserved": "© 2025 All Rights Reserved."
  },
  "home": {
    "heroTitle": "Build AI Without Limits",
    "heroSubtitle": "We build complete GPU infrastructure so you can focus on what really matters — your AI.",
    "getStarted": "Get Started",
    "viewPricing": "View Pricing",
    "foundationTitle": "The Foundation for Your AI Success",
    "foundationSubtitle": "— Powered by GPU Cloud Solutions",
    "foundationDescription": "GMI Cloud provides everything you need to build scalable AI solutions — running entirely on GMI-owned, AI-optimized datacenters. From high-performance inference and containerized ops to on-demand access to top-tier GPUs for training and inference, we control the full stack.",
    "inferenceTitle": "Inference Engine",
    "inferenceDescription1": "GMI Cloud Inference Engine provides developers with the ultra-fast speed and flexible scalability needed to run AI models, designed for extremely low latency, high concurrency processing, and top-tier performance optimization. Whether it's deep learning, natural language processing (NLP), or computer vision (CV), it supports everything perfectly for higher inference efficiency.",
    "inferenceDescription2": "Through GMI Cloud's global GPU node layout, developers can instantly deploy models, scale computing power anytime and anywhere, automatically adjust computing loads, significantly reduce costs and improve performance. Scale elastically on demand to instantly handle high-traffic computing challenges, delivering faster, more stable, and more accurate AI predictions.",
    "inferenceModelsLabel": "Easily run market-leading AI models",
    "clusterTitle": "Cluster Engine",
    "clusterDescription": "GMI Cloud's Cluster Engine is your all-in-one AI cloud management platform, designed to streamline GPU resource coordination, workload scheduling, and cost optimization. Whether you're managing AI training, fine-tuning, or inference workloads, Cluster Engine provides real-time visibility and control.",
    "gpuTitle": "GPUs",
    "gpuDescription": "GMI Cloud offers a diverse portfolio of NVIDIA GPUs, designed for AI/ML training, inference, and data analytics. Our GPU infrastructure is purpose-built for the most demanding AI workloads with ultra-low latency, high memory bandwidth, and enterprise-grade reliability.",
    "caseStudyTitle": "Proven Results",
    "caseStudySubtitle": "Our customers achieve measurable improvements in cost, performance, and time-to-market.",
    "caseStudyQuote": "NexusAI transformed how we deploy AI models. What used to take weeks now takes hours.",
    "costSavings": "Cost Savings",
    "costSavingsDesc": "Average reduction in infrastructure costs",
    "fasterDeployment": "Faster Deployment",
    "fasterDeploymentDesc": "From weeks to hours for new GPU clusters",
    "newsTitle": "Latest Insights",
    "newsSubtitle": "Stay updated with the latest in AI infrastructure and GPU computing.",
    "ctaTitle": "Ready to Deploy AI at Scale?",
    "ctaSubtitle": "Join thousands of companies using NexusAI to power their AI infrastructure. Get started in minutes."
  },
  "faq": {
    "title": "Frequently Asked Questions",
    "subtitle": "Everything you need to know about our GPU infrastructure platform.",
    "items": {
      "q1": {
        "question": "What GPU models do you offer?",
        "answer": "We offer the latest NVIDIA GPUs including H100, A100, and L40S. All clusters are equipped with NVLink and InfiniBand for maximum performance in distributed training and inference workloads."
      },
      "q2": {
        "question": "How quickly can I deploy a new cluster?",
        "answer": "Most clusters can be provisioned within hours, not weeks. Our automated infrastructure allows for rapid scaling based on your needs, with pre-configured environments for popular ML frameworks."
      },
      "q3": {
        "question": "What security certifications do you have?",
        "answer": "We maintain SOC 2 Type II, HIPAA, and GDPR compliance. All data is encrypted at rest and in transit, with optional dedicated tenancy for maximum isolation."
      },
      "q4": {
        "question": "Can I bring my own models?",
        "answer": "Absolutely. Our platform supports any model format including PyTorch, TensorFlow, and ONNX. We also provide optimized serving infrastructure for popular open-source models like Llama, Mistral, and more."
      },
      "q5": {
        "question": "How does billing work?",
        "answer": "We offer flexible pricing including on-demand, reserved capacity, and committed use discounts. You only pay for the compute you use, with transparent pricing and no hidden fees."
      }
    }
  },
  "news": {
    "articles": {
      "article1": {
        "title": "Scaling AI Infrastructure: Lessons from 10,000 GPUs",
        "excerpt": "How enterprise teams are managing GPU clusters at unprecedented scale.",
        "category": "Engineering"
      },
      "article2": {
        "title": "The Future of Inference: H100 vs A100 Benchmarks",
        "excerpt": "Real-world performance comparisons for production AI workloads.",
        "category": "Research"
      },
      "article3": {
        "title": "Cost Optimization Strategies for AI Teams",
        "excerpt": "Practical tips to reduce your GPU infrastructure costs by up to 50%.",
        "category": "Business"
      }
    }
  },
  "cluster": {
    "centralizedManagement": "Centralized Management",
    "centralizedManagementDesc": "Unify your GPU resources across multiple regions and manage workloads from a single, intuitive dashboard.",
    "realtimeDashboard": "Real-Time Dashboard",
    "realtimeDashboardDesc": "Monitor performance metrics, track resource utilization, and get instant alerts for any anomalies in your infrastructure.",
    "accessManagement": "Access Management",
    "accessManagementDesc": "Securely manage user permissions and control access to GPU resources with role-based access controls and team workspaces.",
    "activeGPUs": "Active GPUs",
    "lastUpdated": "Last updated: Just now",
    "avgGPUUsage": "Avg. GPU Usage",
    "activeNodes": "Active Nodes",
    "memoryUsed": "Memory Used",
    "uptime": "Uptime",
    "teamMembers": "Team Members",
    "invite": "+ Invite"
  },
  "gpu": {
    "topTierGPUs": "Top Tier GPUs",
    "topTierGPUsDesc": "Access the latest NVIDIA H100 and A100 GPUs with guaranteed availability and competitive pricing.",
    "infinibandNetworking": "InfiniBand Networking",
    "infinibandNetworkingDesc": "Ultra-low latency interconnects with 400Gb/s bandwidth for distributed training and real-time inference.",
    "secureScalable": "Secure and Scalable",
    "secureScalableDesc": "Enterprise-grade security with SOC 2 Type II, HIPAA, and GDPR compliance. Scale from 1 to 1000+ GPUs."
  },
  "modelLibrary": {
    "heroTitle": "Model Library",
    "heroSubtitle": "Explore our curated library of powerful open-source models and accelerate your AI apps.",
    "filters": {
      "all": "All",
      "llm": "LLM",
      "vision": "Vision",
      "embedding": "Embedding"
    },
    "searchPlaceholder": "Search models...",
    "ctaTitle": "Not sure which product fits your needs? Let's talk.",
    "ctaSubtitle": "Our team is here to help you choose the right infrastructure and models for your AI applications.",
    "types": {
      "textGeneration": "Text Generation",
      "codeGeneration": "Code Generation",
      "imageGeneration": "Image Generation",
      "visionLanguage": "Vision Language",
      "textEmbedding": "Text Embedding"
    }
  },
  "inferenceEngine": {
    "heroTitle": "NexusAI Cloud Inference Engine",
    "heroSubtitle": "Run and scale generative AI models with ease. Deploy pre-trained models for fast, low-latency inference deployment with blazing speed and scalability at scale you can rely on.",
    "cubeLabel": "Inference",
    "cubeSubLabel": "Engine",
    "smarterWayTitle": "A Smarter Way to Inference",
    "features": {
      "deployment": {
        "title": "Rapid Deployment, Zero Hassle",
        "description": "Scale instantly with fully managed infrastructure. Our inference engine comes with all the automation and tooling you need to ship faster, so you can focus on building."
      },
      "efficiency": {
        "title": "Optimized for Efficiency",
        "description": "Advanced GPU optimization and model caching minimize latency while maximizing throughput. Get more inferences per dollar with our intelligent resource allocation."
      }
    },
    "modelShowcase": {
      "title": "Pre-Built AI Models for Fast Inference",
      "subtitle": "Choose from our curated library of pre-optimized models or bring your own",
      "inference": "Inference",
      "viewAllModels": "View all models in our library →",
      "models": {
        "llama2": {
          "name": "Llama 2",
          "description": "Open-source large language model for text generation and chat"
        },
        "stableDiffusion": {
          "name": "Stable Diffusion",
          "description": "State-of-the-art image generation model"
        },
        "mpt7b": {
          "name": "MPT-7B",
          "description": "Efficient transformer model for commercial use"
        }
      }
    },
    "scaling": {
      "kicker": "Auto-Scale",
      "title": "Effortless Scaling for Your AI Workloads",
      "description": "Scale from zero to thousands of concurrent requests automatically. Our intelligent auto-scaling adjusts resources in real-time based on traffic patterns.",
      "dynamicScaling": {
        "title": "Dynamic Scaling",
        "description": "Automatically provisions GPUs as demand increases, scales down during quiet periods."
      },
      "flexibility": {
        "title": "Blazing Flexibility",
        "description": "Choose your minimum and maximum replicas, set custom scaling metrics."
      },
      "cta": "Get Started Now"
    },
    "monitoring": {
      "kicker": "Monitor",
      "title": "Real-Time AI Performance Monitoring",
      "description": "Keep your finger on the pulse with comprehensive dashboards. Track latency, throughput, and error rates in real-time to ensure optimal performance.",
      "totalInferences": "Total Inferences",
      "avgLatency": "Avg Latency",
      "throughput": "Throughput",
      "uptime": "Uptime",
      "costPerInference": "Cost/1K inferences"
    },
    "cta": {
      "title": "Start Inferencing Now",
      "subtitle": "Deploy your first model in minutes. No complex setup required. Just bring your model and start serving predictions at scale."
    },
    "faq": {
      "title": "Frequently asked questions",
      "subtitle": "Get quick answers to common questions about Inference Engine",
      "items": {
        "q1": {
          "question": "What is the NexusAI Cloud Inference Engine?",
          "answer": "The Inference Engine is our managed platform for deploying and scaling AI models in production. It handles all the infrastructure complexity so you can focus on building great AI applications. Deploy pre-trained models or bring your own with minimal configuration."
        },
        "q2": {
          "question": "How fast is deployment and how much latency to expect?",
          "answer": "Deployment typically takes under 2 minutes for pre-built models. For latency, most models achieve sub-100ms inference times, with optimized models like Llama 2 reaching as low as 30-50ms for typical requests. Cold starts are minimized through intelligent caching."
        },
        "q3": {
          "question": "How does it optimize performance and cost?",
          "answer": "Our engine uses GPU batching, model caching, and intelligent request routing to maximize throughput while minimizing costs. Auto-scaling ensures you only pay for what you use, scaling down to zero when there's no traffic."
        },
        "q4": {
          "question": "How does auto-scaling handle fluctuating traffic?",
          "answer": "The auto-scaler monitors request queue depth, latency, and GPU utilization in real-time. When traffic spikes, new replicas spin up in seconds. During quiet periods, resources scale down automatically. You can configure min/max replicas and custom scaling triggers."
        },
        "q5": {
          "question": "Do I get built-in monitoring and operational insights?",
          "answer": "Yes! Every deployment includes a comprehensive dashboard with real-time metrics: latency percentiles, throughput, error rates, GPU utilization, and cost tracking. Set up alerts for anomalies and export data to your observability stack."
        }
      }
    }
  },
  "clusterEngine": {
    "heroTitle": "NexusAI Cloud Cluster Engine",
    "heroSubtitle": "Automated orchestration for AI workloads at scale. Deploy, manage, and monitor GPU clusters with enterprise-grade reliability and performance.",
    "deployNow": "Deploy Now",
    "integrationTitle": "Your AI Control Plane for Cluster and Container Orchestration",
    "integrationSubtitle": "Seamlessly integrate with your existing tools and infrastructure",
    "features": {
      "scaling": {
        "title": "Efficient Scaling",
        "description": "Automatically scale your GPU clusters up or down based on workload demands. Our intelligent orchestration ensures optimal resource utilization."
      },
      "monitoring": {
        "title": "Real-time Monitoring",
        "description": "Monitor every aspect of your cluster with comprehensive dashboards. Track instance health, resource usage, and job progress in real-time."
      },
      "analytics": {
        "title": "Usage Analytics",
        "description": "Gain deep insights into your compute usage patterns. Visualize trends, optimize costs, and make data-driven infrastructure decisions."
      },
      "resources": {
        "title": "Resource Management",
        "description": "Fine-grained control over every GPU, node, and container. Allocate resources precisely where they're needed with our intuitive management interface."
      },
      "security": {
        "title": "Enterprise Security",
        "description": "Bank-grade security for your AI workloads. Role-based access control, encryption at rest and in transit, and comprehensive audit logging."
      }
    },
    "ctaTitle": "Ready to scale your AI infrastructure?",
    "ctaSubtitle": "Get started with Cluster Engine today and experience enterprise-grade orchestration.",
    "darkCtaTitle": "Manage Workloads Effortlessly",
    "darkCtaSubtitle": "Deploy, monitor, and scale your AI workloads with our intuitive Cluster Engine. Built for teams who demand reliability at scale.",
    "faqTitle": "Frequently asked questions",
    "faqSubtitle": "Everything you need to know about Cluster Engine"
  },
  "careers": {
    "heroTitle": "Build the Future of AI with NexusAI",
    "heroSubtitle": "At NexusAI, we're not just building cutting-edge cloud infrastructure — we're shaping the future of AI and high-performance computing. Innovation thrives when brilliant minds have the freedom to push boundaries, which is why we foster a dynamic, fast-paced environment where creativity and expertise drive progress.",
    "joinButton": "Join NexusAI",
    "seePositions": "See Open Positions",
    "valuesKicker": "We're not just offering jobs — we're offering the chance to shape the future of AI.",
    "values": {
      "engineering": {
        "title": "Engineering Excellence",
        "description": "Build complex, cutting-edge technology that push the limits of what's possible. If you're an engineer who thrives on innovation, this is where you can truly test your skills and grow."
      },
      "business": {
        "title": "Business at the Forefront",
        "description": "Join a team of bold thinkers charting a global business from the ground up. We're looking for exceptional problem solvers ready to make a real impact in a rapidly evolving industry."
      },
      "startup": {
        "title": "Startup Energy, Big Ambitions",
        "description": "We move fast, value fresh ideas, and believe in bold leaps. At NexusAI Cloud, proactivity, speed, and innovation aren't just encouraged — they're essential."
      }
    },
    "officesKicker": "Our team is at the forefront of global AI development.",
    "joinTeamTitle": "Join a Global Team of Innovators",
    "joinTeamSubtitle": "We want to bring together bold thinkers from around the world to drive the future of AI and high-performance computing. Our diverse, multicultural team thrives on collaboration, fresh perspectives, and a shared passion for pushing boundaries."
  },
  "partners": {
    "heroTitle": "Join the NexusAI Partner Program",
    "heroSubtitle": "Join a robust ecosystem of technology leaders to drive growth and innovation in AI infrastructure.",
    "stats": {
      "founded": "Founded",
      "partners": "Partners",
      "gpuHours": "GPU Hours"
    },
    "diverseTitle": "Diverse Partners, Tailored Success",
    "whyJoinTitle": "Why Join the NexusAI Partner Program?",
    "benefits": {
      "access": {
        "title": "Exclusive Access",
        "description": "Get priority access to new GPU infrastructure, beta features, and dedicated support channels."
      },
      "marketing": {
        "title": "Co-Marketing",
        "description": "Amplify your brand through joint marketing campaigns, case studies, and partner spotlights."
      },
      "support": {
        "title": "Technical Support",
        "description": "Receive dedicated technical resources, training materials, and priority engineering support."
      }
    },
    "newProductTitle": "New product",
    "faqTitle": "Frequently asked questions"
  },
  "blog": {
    "heroTitle": "NexusAI Blog",
    "heroSubtitle": "Discover the latest news, updates, and insights from our team."
  },
  "models": {
    "deepseekR1": {
      "description": "Open-source reasoning model, rivaling OpenAI o1, excelling in math, coding, and multi-step reasoning."
    },
    "deepseekR1Distill": {
      "description": "Free endpoint to experience powerful reasoning model, this distilled version retains excellent reasoning capabilities."
    },
    "llama33": {
      "description": "Open-source reasoning model, supports multi-language dialogue optimization, specifically tuned for dialogue fluency."
    },
    "free": "Free"
  },
  "languages": {
    "en": "English",
    "zh-TW": "繁體中文",
    "ja": "日本語",
    "ko": "한국어"
  }
}